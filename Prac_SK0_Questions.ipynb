{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Practice Exercise: Scikit-Learn 0\n",
    "### Introduction to Predictive Modeling with Python and Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Objectives\n",
    "\n",
    "As in the [SK0 Tutorial](https://www.featureranking.com/tutorials/machine-learning-tutorials/sk-part-0-introduction-to-machine-learning-with-python-and-scikit-learn/), the objective of this practice notebook is to show you a panoramic view of how to build simple machine learning models using the cleaned \"income data\" from previous data preparation practices using a `holdout` approach. In the previous practices, you cleaned and transformed the raw `income data` and renamed the `income` column as `target` which is defined as:\n",
    "\n",
    "\n",
    "$$\\text{target} = \\begin{cases} 1 & \\text{ if the income exceeds USD 50,000} \\\\ 0 & \\text{ otherwise }\\end{cases}$$\n",
    "\n",
    "Including `target`, the cleaned data consists of 42 columns and 45,222 rows. Each column is numeric and between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 0: Modeling Preparation\n",
    "\n",
    "- Read the clean data `us_census_income_data_clean_encoded.csv` available [here](https://github.com/akmand/datasets). \n",
    "- Randomly sample 5000 rows as it's too big for a short demo (using a random seed of 999).\n",
    "- Split the sampled data as 70% training set and the remaining 30% test set using a random seed of 999. \n",
    "- Remember to separate `target` during the splitting process. \n",
    "- Side question: Why do we need to set a random seed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Verify whether training and test sets have similar proportion of target labels. Why is this step important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "- Fit a nearest neighbor (NN) classifier with $k=5$ neighbors using the Euclidean distance. \n",
    "- Fit the model on the train data and evaluate its performance on the test data using accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "- Extend the previous question by fitting $k=1, 3, 5, 10, 15, 20$ neighbors using the Manhattan and Euclidean distances respectively. \n",
    "- What is the optimal $k$ value for each distance metric? That is, at which $k$, the NN classifier returns the highest accuracy score? **Note:** We will learn later how to perform \"grid search\" to determine the optimal $k$ in later practices.\n",
    "- Which distance metric seems to be better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "- Fit a decision tree classifier with the entropy split criterion and a maximum depth of 5 on the train data, and then evaluate its performance on the test data. \n",
    "- Does it perform better than the \"best\" KNN model from the previous question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "- Fit a random forest classifier with `n_estimators=100` on train data, and then evaluate its performance on the test data. \n",
    "- Does it perform better than the decision tree model in the previous question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Fit a random forest classifier with `n_estimators=250` on train data, and then evaluate its performance on the test data. Does it return a higher accuracy compared to `n_estimators=100`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 7\n",
    "\n",
    "Fit to a Gaussian naive Bayes classifier with a variance smoothing value of $10^{-2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "Fit to a support vector machine with default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9\n",
    "\n",
    "Predict the first three observations of the full cleaned data using the support vector machine built in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10\n",
    "\n",
    "Use `Pandas` to create a confusion matrix for the SVM model.<br>\n",
    "**Hint:** Use pd.crosstab(). <br>\n",
    "**Note:** We will learn how to use other performance evaluation measures using `Scikit-Learn` in upcoming practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11\n",
    "\n",
    "- Which cells in the confusion matrix correspond to `TP` and `TN`? \n",
    "- Calculate\n",
    "> - Accuracy rate\n",
    "> - Error rate\n",
    "> - Precision (across the \"1\" column)\n",
    "> - Recall (across the \"1\" row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "www.featureranking.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
