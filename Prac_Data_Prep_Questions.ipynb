{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Preprocessing Exercises\n",
    "\n",
    "Suppose you are assigned to develop a machine learning model to predict whether an individual earns more than USD 50,000 or less in a year using the 1994 US Census Data. The datasets are sourced from the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml/datasets/Census+Income.\n",
    "\n",
    "The repository provides 5 datasets. However, each dataset is raw and does not come in the form of ABT (Analytic Base Table). The datasets are apparently not ready for predictive modeling.\n",
    "\n",
    "The objective of this notebook is to guide you through the data preprocessing steps on the raw datasets in a sequence of exercises. The expected outcome is \"clean\" data that can be directly fed into *any* machine learning algorithm within the Scikit-Learn Python module. The clean data should look like the dataset used in this [case study](https://www.featureranking.com/tutorials/machine-learning-tutorials/case-study-predicting-income-status/) on our website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The UCI Machine Learning Repository provides five datasets, but only `adult.data`, `adult.test`, and `adult.names` were useful in this project:\n",
    "\n",
    "* `adult.data` and `adult.test` are the training and test datasets respectively. \n",
    "* `adult.names` contains the details of attributes or variables. \n",
    "\n",
    "The training dataset has 32,561 training observations. Meanwhile, the test dataset has 16,281 test observations. Both datasets consist of 14 descriptive features and one target feature called `income`. In this exercise, we combine both training and test data into one as part of data preprocessing. The target feature is defined as below.\n",
    "\n",
    "$$\\text{income} = \\begin{cases} > 50K & \\text{ if the income exceeds USD 50,000} \\\\ \\leq 50K & \\text{ otherwise }\\end{cases}$$\n",
    "\n",
    "The descriptive features below are produced from the `adult.names` file: \n",
    "\n",
    "* **`age`**: continuous.\n",
    "* **`workclass`**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "* **`fnlwgt`**: continuous.\n",
    "* **`education`**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "* **`education-num`**: continuous.\n",
    "* **`marital-status`**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. \n",
    "* **`occupation`**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-*inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "* **`relationship`**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "* **`race`**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "* **`sex`**: Female, Male.\n",
    "* **`capital-gain`**: continuous.\n",
    "* **`capital-loss`**: continuous.\n",
    "* **`hours-per-week`**: continuous.\n",
    "* **`native-country`**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US (Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "Most of the descriptive features are self-explanatory, except `fnlwgt` which stands for “Final Weight” defined by the US Census. The weight is an “estimate of the number of units in the target population that the responding unit represents”. This feature aims to allocate similar weights to people with similar demographic characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**Exercise 0**\n",
    "\n",
    "Read the training and test datasets directly from the data URL's. Also, since the datasets do not contain the feature names, explicitly specify them while loading in the datasets. Once you read in `adultData` and `adultTest` datasets, concatenate them into a single dataset called `df`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Make sure the feature types match the descriptions outlined in the [Data Description](#Data-Description) section. For example, confirm `age` is a numeric feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Calculate the number of missing values for each feature. Does the result surprise you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "In Exercise 2, you should see zero missing value for each feature. This indicates some features are coded with different labels such as \"?\" and \"99999\", instead of `NaN`. To provide a better overview, generate summary statistics of `df`. **Hint**: Use the `describe()` method with `include=np.number` and `include=np.object`. Make sure you have Python 3.6+ for this to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "In Exercise 3, you can see the target feature `income` has four unique values. This *contradicts* the definition of `income` as it should have two only labels: \"<=50K\" and \">50K\". In this exercise, return the unique values of `income`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "In Exercise 4, you should see `income` consists of 4 unique values. The values are `[' <=50K', ' >50K', ' <=50K.', ' >50K.']`. The value contains excessive white space. In this exercise:\n",
    "\n",
    "1. Remove the excessive white space of `income` in `df`.\n",
    "2. Correct the lable of `income` in `df`. In particular, relabel `>50K.` and `<=50K.` to `>50K` and `<=50K` respectively by removing `.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**\n",
    "\n",
    "In Exercise 5, you can see that the raw (or the pre-cleaned) `income` column contained excessive white spaces. Can other categorical features ('workclass', 'education', 'marital-status', 'relationship',  'occupation', 'race', 'sex','native-country') have the same problem? Check which features have excessive white spaces. Remove the white spaces if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative, you can extract the categorical features, excluding `income` into a list.\n",
    "\n",
    "```Python\n",
    "categorical_cols = list(df.columns[df.dtypes == np.object])\n",
    "categorical_cols = [x for x in categorical_cols if x!='income']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**\n",
    "\n",
    "The `workclass`, `occupation`, and `native-country` contain some missing values encoded as \"?\". Check the percentage of \"?\" in each of `workclass`, `occupation`, and `native-country`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**\n",
    "\n",
    "In Exercise 7, you will notice missing values for both of the `workclass` and `occupation` features are about 5.7%. The `native-country` feature contains less than 2% of missing values. Note that the missing data (around more than 90%) are predominantly for the `<=50K` income whereas ~76% of observations pertain to the `<=50K` income at an aggregate level. Therefore, we shall not to impute these missing values but remove them instead as there would be minimal information loss. \n",
    "\n",
    "In this exercise, remove the rows where `workclass=\"?\"`, `occupation=\"?\"` and `native-country=\"?\"`. Check that the number of observations reduces from 48,842 to 45,222."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9**\n",
    "\n",
    "In Exercise 7, notice that `native-country` is too granular and unbalanced. That is, close to 90% of `native-country` is \"United-States\" and the remaining 10% is made up of 40 different countries. The granularity (or the large cardinality) would yield a large number of columns when we encode `native-country`. You should also notice `race` exhibits the same problem where `white` accounts more than 75% of instances. In this exercise, \n",
    "\n",
    "1. For `native-country`, relabel all other countries as \"Other\" except \"United-States\". \n",
    "2. Likewise, relabel all other `race` as \"Other\" except \"White\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10**\n",
    "\n",
    "Recall that `fnlwgt` stands for \"Final Weight\" defined by the US Census. The weight is an \"estimate of the number of units in the target population that the responding unit represents\" This feature aims to allocate similar weights to people with similar demographic characteristics. In short, `fnlwght` has no predictive power. \n",
    "\n",
    "In this exercise, remove `fnlwgt` from `df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11**\n",
    "\n",
    "We suspect `education` and `education-num` might carry the same information. If they represent the same information, we should remove one of them (why?). To see this, run `len(df['education'].unique())` and `len(df['education-num'].unique())`. Both should give you 16 unique values. To vindicate our suspection, make sure `education` and `education-num` indeed represent the same information. Then, drop `education` from `df`. **Hint**: try `pd.pivot_table`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "In the previous exercises, we have performed heaps of data wrangling on target and categorical features. Let's focus on the continuous/numeric features: `age`, `capital-gain`, `capital-loss` and `hours-per-week`. Based on the summary statistics (in Exercise 2), `age` ranges 17 to 90 years old with a mean of 38.64. Therefore, we can conclude `age` has a reasonable range of values and hence requires no data wrangling. However, `hours-per-week` ranges from 1 to 99 hours with a mean of 40. We suspect 99 is used to label the \"missing\" instance of `hours-per-week`. On the other hand, it is still possible to work more than 90 hours per week. Therefore, we shall not preprocess it further. To check this, you can find the second largest value of `hours-per-week` is 98. Next, `capital-gain` ranges from 0 to 99,999 and we suspect that the missing observations are hence labeled as \"99999\". Is that true? We run the following code and find that a capital gain value of 99,999 always returns a higher income earner, we conjecture it might be a useful predictive value and hence we shall not remove the observations with this value.\n",
    "```Python\n",
    "df.loc[df['capital-gain'] == 99999.000000, 'income'].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 12**\n",
    "\n",
    "We suspect that `capital-loss =  − capital-gain` because an individual can either pay any capital gain or claim for capital loss to reduce capital gain in future (TaxBracket.org, 2017). Another possibility is to pay neither gain nor loss. Hence, it is more reasonable to record gain or loss as a single variable, rather than having them separate. Before defining such new variable, define a \"mask_both\" to verify that no observations were recorded as both positive capital-gain and capital-loss values. You should see there is zero count for \"True\" in the \"mask_both\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 13**\n",
    "\n",
    "In light of Exercise 12, define a variable named `capital` which is given as `capital-gain - capital-loss`. Then remove `capital-gain` and `capital-loss` from `df`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14**\n",
    "\n",
    "As a good practice, the target feature should be the *last column* in the data frame. Move the target feature `income` to the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 15**\n",
    "\n",
    "Print the shape of `df` and a list of the columns at this point. Also randomly sample 4 rows with a random state of 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16**\n",
    "\n",
    "Remove the `income` feature from the full dataset and call it `target`. Get a distribution of the values of the target feature (that is, value counts). Name the rest of the features `Data`, which are your descriptive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 17**\n",
    "\n",
    "Label-encode the target feature so that the positive class is \">50K\" and it is encoded as \"1\". The negative class should be encoded as \"0\". Confirm correctness of your label-encoding by getting a value counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 18**\n",
    "\n",
    "Get a list of all descriptive categorical features. Display value counts for each one of these features. Comment on which feature(s) appear to be ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 19**\n",
    "\n",
    "So all the categorical features appear to be nominal. Perform one-hot encoding for all the descriptive categorical features and call this encoded data frame as `Data_encoded`. If a categorical descriptive feature has only 2 levels, encode it with only one binary variable. For other categorical features (with more than 2 levels), use regular one-hot-encoding (where number of binary variables are equal to the number of distinct levels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 20**\n",
    "\n",
    "How many descriptive features are there now? Randomly sample 4 rows with a random state of 11. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 21**\n",
    "\n",
    "After encoding all the categorical features, we end up with a data frame that is all numerical. Get a description of `Data_encoded` with include='all' option.\n",
    "\n",
    "Next, perform a range normalization of the descriptive features using `MinMaxScaler()` method within `preprocessing` submodule of Scikit-Learn, and call it `Data_encoded_norm_numpy`. But make sure you keep `Data_encoded` around to keep track of column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 22**\n",
    "\n",
    "Pay attention that the output of the scaler is a NumPy array, so all the column names are lost. That's why you kept a copy of `Data_encoded` before scaling so that you can recover the column names. Define a new Pandas data frame called `Data_encoded_norm_df` from `Data_encoded_norm_numpy` with the column names of `Data_encoded`. \n",
    "\n",
    "Then have another look at the descriptive features after scaling by randomly sampling 4 rows with a random state of 11 from `Data_encoded_norm_df`. \n",
    "\n",
    "Finally, get the shape and a description of `Data_encoded_norm_df` with include='all' option. Observe that everything is now between 0 and 1 and that binary features are still kept as binary after the min-max scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 23**\n",
    "\n",
    "Define a new data frame called `df_clean` which is the combination of the normalized and scaled descriptive features and the target feature with the target feature as the last column. **Hint:** Use `assign()`, but make sure you use the `.values` on the `target` feature. \n",
    "\n",
    "Randomly sample 4 rows from `df_clean` with a random state of 11. \n",
    "\n",
    "Write the clean dataset `df_clean` to a CSV file called `df_clean.csv`.\n",
    "\n",
    "Finally, open this CSV file with your favorite CSV program (Excel should work fine) and have a look to make sure everything looks OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 24**\n",
    "\n",
    "Scikit-Learn models require input data separately as the set of descriptive features and the target feature respectively. In addition, Scikit-Learn models require all data to be NumPy arrays (2-dimensional arrays for descriptive features and 1-dimensional arrays for the target feature). How would you go about specifying these two inputs with the objects you have defined so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further References\n",
    "\n",
    "For more information on the entire data preprocessing process, please refer to the Data Prep lecture notes (on Chapters 2 and 3) and the [Data Prep tutorial](https://www.featureranking.com/tutorials/machine-learning-tutorials/data-preparation-for-machine-learning/) on our website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "www.featureranking.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
