{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercise: Scikit-Learn 3\n",
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "As in the [SK3 Tutorial](https://www.featureranking.com/tutorials/machine-learning-tutorials/sk-part-3-model-evaluation/), the objective of this practice notebook is to illustrate how you can evaluate machine learning algorithms using various performance metrics. We will show two examples of this: one for classification and one for regression.\n",
    "\n",
    "You will use **stratified 5-fold cross-validation with 2 repetitions** during training. For testing, you will use the fine-tuned model for prediction **without** any cross-validation for simplicity.\n",
    "\n",
    "In `GridSearchCV()`, try setting `n_jobs` to -2 for shorter run times with parallel processing. Here, -2 means use all core except 1. See [SK5 Tutorial](https://www.featureranking.com/tutorials/machine-learning-tutorials/sk-part-5-advanced-topics-pipelines-statistical-model-comparison-and-model-deployment/) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Evaluating Classifiers\n",
    "\n",
    "In the previous practices, you cleaned and transformed the raw `income data` and renamed the `income` column as `target` (with high income being the positive class). Including `target`, the cleaned data consists of 42 columns and 45,222 rows. Each column is numeric and between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0: Modeling Preparation\n",
    "\n",
    "- Read in the clean data `us_census_income_data_clean_encoded.csv` on GitHub [here](https://github.com/akmand/datasets). \n",
    "- Randomly sample 5000 rows using a random seed of 999.\n",
    "- Split the sampled data as 70% training set and the remaining 30% test set using a random seed of 999. \n",
    "- Remember to separate `target` during the splitting process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Get a value counts of the target feature levels in the sample data. Do you see a class imbalance problem? In this case, which performance metrics would you prefer?\n",
    "\n",
    "**HINT:**\n",
    "\n",
    "For a list of scorers as a **string** that you can pass into `cross_val_score()` or `GridSearchCV()` methods, please try this:\n",
    "```Python\n",
    "from sklearn import metrics \n",
    "metrics.SCORERS.keys()\n",
    "```\n",
    "\n",
    "`Scikit-Learn` has a module named `metrics` which contains different performance metrics for classifiers and regressors. For a list of metrics **methods** that you can use, please see official Scikit-Learn documentation on model evaluation [here](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Fit and fine-tune a DT model using the **train** data. For fine-tuning, consider max_depth values in {3, 5, 7, 10} and min_samples_split values in {2, 5, 15, 20}. Display the best parameter values and the best estimator found during the grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Get the predictions for the test data using the best estimator. You can achieve this via the following:\n",
    "```Python\n",
    "t_pred = gs_DT.predict(D_test)\n",
    "```\n",
    "Using the predictions on the **test** data, display the confusion matrix. In addition, compute the following metrics:\n",
    "1. Accuracy rate\n",
    "2. Error (misclassification) rate\n",
    "3. Precision\n",
    "4. Recall (TPR)\n",
    "5. F1-Score\n",
    "6. AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Visualize an ROC curve by calculating prediction scores using the `predict_proba` method in `Scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluating Regressors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0: Modeling Preparation\n",
    "    \n",
    "For evaluating regressors, we will use the **diamonds** dataset from Prac_SK1. On Canvas, you will see a CSV called 'diamonds_clean_5000.csv'. This is preprocessed diamonds dataset with a random sample of 5000 instances. Read in this dataset and display 5 random instances. Split this data as 70% training set and the remaining 30% test set using a random seed of 999. The target response is the `price`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Fit and fine-tune a DT regressor model using the **train** data. For fine-tuning, consider max_depth values in {10, 20, 30, 40} and min_samples_split values in {15, 25, 35}. For scoring, use **MSE (mean squared error)**. Display the best parameter values and the best estimator found during the grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "    \n",
    "Get the predictions for the test data using the best estimator. You can achieve this via the following:\n",
    "```Python\n",
    "t_pred = gs_DT_regressor.predict(D_test)\n",
    "```\n",
    "Using the predictions on the **test** data, compute the following metrics:\n",
    "1. MSE\n",
    "2. RMSE\n",
    "3. Mean absolute error (MAE)\n",
    "4. $R^2$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Create a histogram of residuals for your DT model. How does it look in terms of shape and spread?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (Optional)\n",
    "\n",
    "In Exercise 2, we obtain an MSE around 870 USD. Without any domain knowledge on diamond prices, how do we conclude whether the range of prediction errors (i.e., residuals) is \"reasonable\"? Let's use standardised residuals $e$ defined as follows: \n",
    "\n",
    "$$e = \\frac{\\varepsilon-\\bar{\\varepsilon}}{\\sigma(\\varepsilon)}$$\n",
    "\n",
    "where residuals are denoted by $\\varepsilon$ with $\\bar{\\varepsilon}$ and $\\sigma$ denoting their mean and standard deviation respectively. \n",
    "\n",
    "**HINT**\n",
    "\n",
    "Consider plotting a histogram of standardised errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "www.featureranking.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
