{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice Exercise: Scikit-Learn 4\n",
    "### Cross-Validation and Hyper-parameter TuningÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objectives\n",
    "\n",
    "As in the [SK4 Tutorial](https://www.featureranking.com/tutorials/machine-learning-tutorials/sk-part-4-cross-validation-and-hyperparameter-tuning/), the objective of this practice notebook is to illustrate how you can perform cross-validation and hyper-parameter tuning using `Scikit-Learn`. You will also perform paired t-tests to determine statistically significant performance results, as explained [here](https://www.featureranking.com/tutorials/machine-learning-tutorials/sk-part-5-advanced-topics-pipelines-statistical-model-comparison-and-model-deployment/#4). \n",
    "\n",
    "You will be using the cleaned \"income data\" dataset. In the previous practices, you cleaned and transformed the raw `income data` and renamed the `income` column as `target` (with high income being the positive class). Including `target`, the cleaned data consists of 42 columns and 45,222 rows. Each column is numeric and between 0 and 1.\n",
    "\n",
    "In `GridSearchCV()`, try setting `n_jobs` to -2 for shorter run times with parallel processing. Here, -2 means use all core except 1. See [SK5 Tutorial](https://www.featureranking.com/tutorials/machine-learning-tutorials/sk-part-5-advanced-topics-pipelines-statistical-model-comparison-and-model-deployment/) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- For ease of computation, you will first randomly sample 5,000 rows from this dataset. \n",
    "- You will then split this sample into 70% train and 30% test datasets. \n",
    "- You will use the first set for **training**: you will fine tune your models using this train data. \n",
    "- You will use the second set for **testing**: you will perform cross-validation in a paired fashion using the test data and then you will compare the performance of your tuned models using a paired t-test.\n",
    "- You will use **stratified 5-fold cross-validation with no repetitions** during both training and testing.\n",
    "- For scoring, you will use AUC, that is, \"area under the ROC curve\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Bookkeeping\n",
    "\n",
    "- Define a variable called `num_samples` and set it to 5000. You will use this variable when sampling a smaller subset of the full set of instances.\n",
    "- Define a variable called `num_features` and set it to 10. Prior to fitting any models, you will perform feature selection by making use of this `num_features` variable.\n",
    "- Define a variable called `scoring_metric` and set it to to `'roc_auc'`. You will set `scoring` option in all `cross_val_score()` functions to this `scoring_metric` variable.\n",
    "\n",
    "You can achieve these by running the code chunk below:\n",
    "```Python\n",
    "import numpy as np\n",
    "np.random.seed(999)\n",
    "num_samples = 5000\n",
    "scoring_metric = 'roc_auc'\n",
    "num_features = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0: Modeling Preparation\n",
    "\n",
    "- Read in the clean data `us_census_income_data_clean_encoded.csv` on GitHub [here](https://github.com/akmand/datasets). \n",
    "- Randomly sample 5000 rows using a random seed of 999.\n",
    "- Split the sampled data as 70% training set and the remaining 30% test set using a random seed of 999. \n",
    "- Remember to separate `target` during the splitting process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Determine the most important 10 features as selected by Random Forest Importance (RFI) using the **training** data and visualize them.\n",
    "\n",
    "In the rest of these exercises, you will use these selected features and not the whole set of descriptive features. You can achieve this as below:\n",
    "```Python\n",
    "D_Train_fs = D_train[:, fs_indices_rfi]\n",
    "D_Test_fs  = D_test[:, fs_indices_rfi]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Fit and fine-tune a KNN model using the **train** data. For fine-tuning, consider K values in {1, 5, 10, 15, 20} and p values in {1, 2}. Also visualize the tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Fit and fine-tune a DT model using the **train** data. For fine-tuning, consider max_depth values in {3, 5, 7, 10, 12} and min_samples_split values in {2, 5, 15, 20, 25}. Also visualize the tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Fit and fine-tune a NB model using the **train** data. For fine-tuning, consider var_smoothing values in `np.logspace(1,-2, num=50)`. Also visualize the tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Fit and fine-tune a Random Forest model using the **train** data. For fine-tuning, consider n_estimators values in {100, 250, 500} and max_depth values in {3, 5, 7, 10, 12}. Also visualize the tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "What we would like to do now is to \"fit\" each tuned classifier (with their best set of hyperparameter values) on the **test** data in a cross-validated fashion to figure out which (tuned) classifier performs the best. This way, we will be measuring performance of the tuned classifiers on data that they did not \"see\" previously.\n",
    "\n",
    "Since cross validation itself is a random process, we would like to perform pairwise t-tests to determine if any difference between the performance of any two (tuned) classifiers is statistically significant. Specifically, we first perform 5-fold stratified cross-validation (without any repetitions) on each (tuned) classifier where we use the same seed in each of the four cross-validation runs. Second, we conduct a paired t-test for the AUC score between the following (tuned) classifier combinations.\n",
    "\n",
    "For this question, perform the procedures discussed above and decide if any one the tuned classifiers is statistically better than the rest at a 95% significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Rerun your notebook with different number of features (e.g., 5, 15, 20). Also try with no feature selection at all (set num_features=41).\n",
    "Comment if your results improve. Which number of features seem to work best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "www.featureranking.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
